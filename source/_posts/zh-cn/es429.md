---
title: 妥善解决es接口触发熔断问题
date: 2024-08-16
updated:
keywords:
slug: es429
cover: /image/es429.png
top_image:
comments: false
maincolor:
categories:
  - ai 技术
tags:
  - elasticsearch
typora-root-url: ./es429
description:
---



# 1、概述

在工作中，遇到 es 接口产生的如下报错：

```
"elastic: Error 429 (Too Many Requests): [parent] Data too large，data for [<http_request>] would be [21877678854/19.6gb] which is larger than the limit of [20401094656/19gb]，real usage:[21077664576/19.6gb]，new bytes reserved:[13478/13.1kb]，usages[request 3/Gb. fielddata=4395405/4.1mb.in_flight_reauests=13478/13.1kb model_inference=0/0b
```

仔细一看，是因为节点内存不足，触发了熔断。今天就来记录一下，发生这种事情该如何处理

当 Elasticsearch（ES）接口触发熔断时，通常是因为系统内存使用过高，超过了设定的阈值。熔断机制（Circuit Breaker）是为了防止内存溢出（OutOfMemoryError）而设计的

# 2、熔断机制

Elasticsearch 提供了多种熔断器，用于监控和限制内存使用：

- **Field Data Breaker**：限制字段数据的内存使用。
- **Request Breaker**：限制请求处理的内存使用。
- **In-flight Request Breaker**：限制正在处理的请求的内存使用。
- **Parent Breaker**：限制所有子熔断器的总内存使用。

而我们发现的熔断类型是 Request Breaker



# 3、检查并解决

### 1. 检查熔断日志

当熔断器触发时，Elasticsearch 会在日志中记录相关信息。检查日志以确定哪个熔断器被触发以及具体的内存使用情况：

```bash
tail -f /var/log/elasticsearch/elasticsearch.log
```



### 2. 判断原因

根据日志信息，发现节点在不断地进行GC操作，是es 进程分配的堆内存使用率非常高，20G的堆内存被用掉19G，导致需要不断GC释放内存，并且导致很多请求直接被熔断器直接拦截返回了。

```
[2024-08-02T10:39:46,955][INFO] attempting to trigger G1GC due to high heap usage [20405447336]
```



### 3. 查看集群整体情况

使用kibana  `GET /_cat/nodes?v` 查看集群所有节点的内存和堆内存的使用率，发现主节点的内存使用率非常高

![](./esnode.jpg)

 内存不足，会导致存储在内存里的倒排索引空间不足，无法进行正常的读写



### 5. 增加硬件资源

内存使用持续过高，可以考虑增加硬件资源，如增加内存或添加更多节点以分散负载。

在联系运维同事给集群增加es节点，直接使用之前部署es 的 ansible 脚本，对 elasticsearch.yaml 文件进行以下设置：

- **cluster.name**：设置为现有集群的名称。
- **node.name**：为新节点指定一个唯一的名称。
- **network.host**：设置为新节点的 IP 地址。
- **discovery.seed_hosts**：列出现有集群中的一个或多个节点的 IP 地址，以便新节点可以发现并加入集群

运行es 进程，等待集群的自动发现把这个节点加入集群就可以了。



当集群检测到有新的节点加入，就触发分片自动迁移机制，重新分配分片，达到到集群的数据平衡。



### 6. 监控和预警

本次的事故是由于运维同事没有及时管理好es集群的资源，做到及时告警给到业务
