---
title: 妥善解决es接口触发熔断问题
date: 2024-08-16
updated:
keywords:
slug: es429
cover: /image/es429.png
top_image:
comments: false
maincolor:
categories:
  - ai 技术
tags:
  - elasticsearch
typora-root-url: ./es429
description:
---

# 1、概述

在工作中，遇到 es 接口产生的如下报错：

```
"elastic: Error 429 (Too Many Requests): [parent] Data too large，data for [<http_request>] would be [21877678854/19.6gb] which is larger than the limit of [20401094656/19gb]，real usage:[21077664576/19.6gb]，new bytes reserved:[13478/13.1kb]，usages[request 3/Gb. fielddata=4395405/4.1mb.in_flight_reauests=13478/13.1kb model_inference=0/0b
```

仔细一看，是因为 es 节点分配的内存不足，触发了熔断。今天就来记录一下，发生这种事情该如何处理

# 2、熔断机制

Elasticsearch 提供了多种熔断器，用于监控和限制内存使用：

- **Field Data Breaker**：限制字段数据的内存使用。
- **Request Breaker**：限制请求处理的内存使用。
- **In-flight Request Breaker**：限制正在处理的请求的内存使用。
- **Parent Breaker**：限制 es 节点的总内存的使用。

而我们发现的熔断类型是 Parent Breaker，也就是限制总内存的断路器，一般由于集群中总索引量增长超过集群承载，线上的查询请求量也随之增加，导致所需的内存过大。

我们在部署 es 集群的时候，通常需要给 es 节点配置 JVM 堆内存大小，用于 es 进程的索引常驻内存、搜索结果缓存等，通过设置 jvm.options 文件里的堆内存大小可以调整：

```conf
-Xms20g  # 堆内存初始值
-Xmx20g  # 堆内存最大值

# 锁定堆内存
bootstrap.memory_lock: true
# 最大锁定内存值
node.max_memory_lock: 15g
```

通过调整堆内存的上限，使得节点能承受更多的查询缓存，以及更多的索引量，一般建议

> > > > > > > Stashed changes

# 3、检查并解决

### 1. 检查熔断日志

当熔断器触发时，Elasticsearch 会在日志中记录相关信息。检查日志以确定哪个熔断器被触发以及具体的内存使用情况：

```bash
tail -f /var/log/elasticsearch/elasticsearch.log
```

### 2. 判断原因

根据日志信息，发现节点在不断地进行 GC 操作，是 es 进程分配的堆内存使用率非常高，20G 的堆内存被用掉 19G，导致需要不断 GC 释放内存，并且导致很多请求直接被熔断器直接拦截返回了。

```
[2024-08-02T10:39:46,955][INFO] attempting to trigger G1GC due to high heap usage [20405447336]
```

### 3. 查看集群整体情况

使用 kibana `GET /_cat/nodes?v` 查看集群所有节点的内存和堆内存的使用率，发现主节点的内存使用率非常高

![](./esnode.jpg)

内存不足，会导致存储在内存里的倒排索引空间不足，无法进行正常的读写

### 5. 增加硬件资源

1、增加集群节点
内存使用持续过高，可以考虑增加硬件资源，如增加内存或添加更多节点以分散负载。

在联系运维同事给集群增加 es 节点，直接使用之前部署 es 的 ansible 脚本，对 elasticsearch.yaml 文件进行以下设置：

- **cluster.name**：设置为现有集群的名称。
- **node.name**：为新节点指定一个唯一的名称。
- **network.host**：设置为新节点的 IP 地址。
- **discovery.seed_hosts**：列出现有集群中的一个或多个节点的 IP 地址，以便新节点可以发现并加入集群

运行 es 进程，等待集群的自动发现把这个节点加入集群就可以了。

当集群检测到有新的节点加入，就触发分片自动迁移机制，重新分配分片，达到到集群的数据平衡。

2、给现有节点增加 heap 内存
找到 es 配置文件中的 `-Xmx`，提升它的内存上限
每个节点都修改，最后再重启集群，应用设置就生效了。

### 6. 监控和预警

本次的事故是由于运维同事没有及时管理好 es 集群的资源，做到及时告警给到业务
